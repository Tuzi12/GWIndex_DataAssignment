{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "We have a Python script, which takes input data, computes weights and assigns them to respondents. \n",
    "As is often the case, the data must be cleaned first. It’s great if you write down how the dataset \n",
    "changes after each step of data cleaning. You can use whatever tools or libraries you like, the goal is to use the optimal \n",
    "solution. \n",
    "\n",
    "## Data wrangling\n",
    "You are going to work with data stored in `input_data.csv.zip`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tdescribe the data: what values it mostly contains, what is the size of the dataset etc.\n",
    "\n",
    "Given the size of the file, I can not read the whole file into the memory to check the size. So I read the first 500 rows just to get a rough idea of what the data look like. It has 2501 columns, mainly integers of 0 and 1. Two columns are float numbers, and one is object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2501)\n",
      "float64       2\n",
      "int64      2498\n",
      "object        1\n",
      "dtype: int64\n",
      "  respondent_index  dxllm  cmeyg  savjf  xxjbt  joprf  unycm  gyykp  ruqoz  \\\n",
      "0       gwiindex-0      1      0      0      0      0      1      1      1   \n",
      "1       gwiindex-1      1      1      0      0      1      1      0      0   \n",
      "2       gwiindex-2      1      1      0      1      0      1      1      0   \n",
      "3       gwiindex-3      1      0      1      0      0      1      0      1   \n",
      "4       gwiindex-4      1      0      1      1      1      1      1      0   \n",
      "\n",
      "   rlknj  ...    hivqb  rvjcd  olrgb  rodxr  jsrni  uxiku  uwcpb  nhxrx  \\\n",
      "0      0  ...        0      1      0      1      0      1      0      0   \n",
      "1      0  ...        0      1      1      0      1      0      0      1   \n",
      "2      0  ...        0      1      1      0      0      1      0      0   \n",
      "3      0  ...        1      1      1      0      0      1      1      1   \n",
      "4      1  ...        0      0      1      0      1      0      0      1   \n",
      "\n",
      "   mmvwd  iqgre  \n",
      "0      1      0  \n",
      "1      1      0  \n",
      "2      0      0  \n",
      "3      1      1  \n",
      "4      1      1  \n",
      "\n",
      "[5 rows x 2501 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "CS = 20000\n",
    "PATH = r'C:\\Jian temp\\GWIndex'\n",
    "orig_filename = r'input_data_orig.csv'\n",
    "q3filename = r'q3_column.csv'\n",
    "new_filename= r'input_data_new.csv'\n",
    "result_filename = r'result_data.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(PATH, orig_filename), nrows=500)\n",
    "print(df.shape)\n",
    "print(df.get_dtype_counts())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tdrop all duplicate respondents based on `respondent_index` column (keep the first occurrence of the respondent)\n",
    "\n",
    "Since the file is too big to read into the memory, I cannot use pandas DataFrame.drop_duplicates([‘respondent_index’], keep=’first’, inplace=True) to remove the duplicated entries. But if I read the file into chunks, I can only remove duplicates within the chunksize such as 20,000 rows. So finally, I read the whole file in as a csv file line by line, check whether there are duplicate lines, then write out lines that are unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH, orig_filename), 'r') as in_file, open(os.path.join(PATH, new_filename), 'w') as out_file:\n",
    "    seen = set()\n",
    "    for line in in_file:\n",
    "        if line in seen:\n",
    "            continue\n",
    "        seen.add(line)\n",
    "        out_file.write(line)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tWe will compute weighting based on the `q2` (gender) and `q4` (age group) column. Drop all respondents having NaNs in these columns. Do you know why this could happen and what the possible solutions are?\n",
    "\n",
    "Use pandas DataFrame[‘q2’].isnull().sum() function to check whether there is any NaNs in that column, the answer is 50, and 51 NaNs in age group. It could be that particular age group has got no response. Possible solution is to go back to check the survey design to see whether do we need to modify that age group value or should we just exclude this age group or replace the values with mean value or other values.\n",
    "\n",
    "As said in previous section, the data has to be read in chuncks due to the memory limit, so I will define a function to read the whole data and another function to process the data by chunks and finally ouput the result into a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19499, 2)\n",
      "9499 ****** new data\n",
      "10000 ****** new data\n",
      "0 ****** new data\n",
      "0 ****** new data\n",
      "0 ****** new data\n",
      "0 ****** new data\n"
     ]
    }
   ],
   "source": [
    "def read_data(path, filename, cs):\n",
    "    yield pd.read_csv(os.path.join(path, new_filename), chunksize=cs)\n",
    "    \n",
    "q3col = pd.read_csv(os.path.join(PATH, q3filename)) \n",
    "q3col.columns = ['respondent_index', 'q3']\n",
    "print(q3col.shape)\n",
    "\n",
    "def process_datachunks(data, q3data):\n",
    "    num = 0        \n",
    "    for chunks in data:\n",
    "        for chunk in chunks:\n",
    "            if chunk['q2'].isnull().sum() > 0:\n",
    "                chunk.dropna(subset=['q2'], inplace=True)\n",
    "            if chunk['q4'].isnull().sum() > 0:\n",
    "                chunk.dropna(subset=['q4'], inplace=True)\n",
    "            new_data = pd.merge(chunk, q3data, on='respondent_index')\n",
    "            print(len(new_data.index), '****** new data')\n",
    "            if len(new_data.index) >= 1 and 0 == num:\n",
    "                new_data.to_csv(os.path.join(PATH, result_filename), index=False)\n",
    "                num +=1\n",
    "            elif len(new_data.index) >= 1 and num > 0:\n",
    "                new_data.to_csv(os.path.join(PATH, result_filename), mode='a', header=False, index=False)\n",
    "                \n",
    "chunks = read_data(PATH, new_filename, CS)\n",
    "process_datachunks(chunks, q3col)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tThere is a `q3_column.csv` file in the directory. Download it and merge/join it on the index to the original dataframe. Are there more variants of (a database) merge/joins? \n",
    "\n",
    "After joining the ‘q3_column.csv ’to the original dataframe, the shape of the dataframe downsize to (19499, 2502), that is, 19499 rows and 2502 columns.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
